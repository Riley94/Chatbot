{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import random\n",
    "from spacy.training import Example\n",
    "import io\n",
    "import re\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        medication\n",
      "0    acetaminophen\n",
      "1        acyclovir\n",
      "2         Adderall\n",
      "3        albuterol\n",
      "4      alendronate\n",
      "..             ...\n",
      "375    Zithromycin\n",
      "376         Zoloft\n",
      "377       Zolpidem\n",
      "378        Zovirax\n",
      "379         Zyrtec\n",
      "\n",
      "[380 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# if csv exists, load it\n",
    "if os.path.exists('../clean_data/medications.csv'):\n",
    "    medications = pd.read_csv('../clean_data/medications.csv')\n",
    "# otherwise, scrape the data\n",
    "else:\n",
    "    url = \"https://healthy.kaiserpermanente.org/health-wellness/drug-encyclopedia.\"\n",
    "\n",
    "    medications = []\n",
    "\n",
    "    # iterate from 'a' to 'z'\n",
    "    for letter in range(97, 123):\n",
    "        url = url + chr(letter)\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        for li in soup.select(\".drug-column-4\"):\n",
    "            medications.append(li.text)\n",
    "        \n",
    "        url = \"https://healthy.kaiserpermanente.org/health-wellness/drug-encyclopedia.\"\n",
    "\n",
    "    # split by new line\n",
    "    medications = [medication.split('\\n') for medication in medications]\n",
    "    # flatten and remove empty strings\n",
    "    medications = [medication for sublist in medications for medication in sublist if medication != '']\n",
    "    medications = pd.DataFrame(medications, columns=['medication'])\n",
    "    medications.to_csv('../clean_data/medications.csv', index=False)\n",
    "\n",
    "\n",
    "print(medications)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "380 medication names to randomize NER data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the scope of the application\n",
    "SCOPES = ['https://www.googleapis.com/auth/drive']\n",
    "# Function to authenticate and create the service\n",
    "def create_service():\n",
    "    creds = None\n",
    "    # The file token.pickle stores the user's access and refresh tokens.\n",
    "    if os.path.exists('token.pickle'):\n",
    "        with open('token.pickle', 'rb') as token:\n",
    "            creds = pickle.load(token)\n",
    "    # If there are no (valid) credentials available, let the user log in.\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file('credentials.json', SCOPES)\n",
    "            creds = flow.run_local_server(port=0)\n",
    "        # Save the credentials for the next run\n",
    "        with open('token.pickle', 'wb') as token:\n",
    "            pickle.dump(creds, token)\n",
    "    \n",
    "    service = build('drive', 'v3', credentials=creds)\n",
    "    return service\n",
    "\n",
    "# Function to list files in a given folder ID\n",
    "def list_files_in_folder(service, folder_id):\n",
    "    results = service.files().list(q=f\"'{folder_id}' in parents\", fields=\"nextPageToken, files(id, name)\").execute()\n",
    "    items = results.get('files', [])\n",
    "    return items\n",
    "\n",
    "# Function to download a file\n",
    "def download_or_export_file(service, file_id, file_name, mime_type):\n",
    "    try:\n",
    "        # Check if the file is a Google Doc by its MIME type\n",
    "        if mime_type.startswith('application/vnd.google-apps.'):\n",
    "            # Define export MIME type for Google Docs (e.g., 'application/pdf' for Google Docs)\n",
    "            if mime_type == 'application/vnd.google-apps.document':\n",
    "                export_mime_type = 'application/pdf'\n",
    "                file_name += '.pdf'  # Append appropriate file extension\n",
    "            elif mime_type == 'application/vnd.google-apps.spreadsheet':\n",
    "                export_mime_type = 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'\n",
    "                file_name += '.xlsx'  # Append appropriate file extension\n",
    "            elif mime_type == 'application/vnd.google-apps.presentation':\n",
    "                export_mime_type = 'application/vnd.openxmlformats-officedocument.presentationml.presentation'\n",
    "                file_name += '.pptx'  # Append appropriate file extension\n",
    "            else:\n",
    "                # Default to PDF for other Google Apps documents\n",
    "                export_mime_type = 'application/pdf'\n",
    "                file_name += '.pdf'\n",
    "            \n",
    "            request = service.files().export_media(fileId=file_id, mimeType=export_mime_type)\n",
    "        else:\n",
    "            # For binary files, use the get_media method\n",
    "            request = service.files().get_media(fileId=file_id)\n",
    "        \n",
    "        # Perform the download or export\n",
    "        fh = io.BytesIO()\n",
    "        downloader = MediaIoBaseDownload(fh, request)\n",
    "        done = False\n",
    "        while not done:\n",
    "            status, done = downloader.next_chunk()\n",
    "            print(f\"Download {int(status.progress() * 100)}%.\")\n",
    "        \n",
    "        # Write the file's contents to a local file\n",
    "        with open(file_name, 'wb') as f:\n",
    "            f.write(fh.getbuffer())\n",
    "        print(f\"File '{file_name}' downloaded successfully.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "def find_folders_by_name(service, folder_name):\n",
    "    \"\"\"Find folders by name and return their IDs.\"\"\"\n",
    "    query = f\"mimeType='application/vnd.google-apps.folder' and name='{folder_name}'\"\n",
    "    response = service.files().list(q=query, spaces='drive', fields='files(id, name)').execute()\n",
    "    return response.get('files', [])\n",
    "\n",
    "def find_subfolder_id(service, parent_folder_id, subfolder_name):\n",
    "    \"\"\"Find a specific subfolder within a parent folder.\"\"\"\n",
    "    query = f\"'{parent_folder_id}' in parents and mimeType='application/vnd.google-apps.folder' and name='{subfolder_name}'\"\n",
    "    response = service.files().list(q=query, spaces='drive', fields='files(id, name)').execute()\n",
    "    files = response.get('files', [])\n",
    "    if files:\n",
    "        return files[0]['id']  # Return the ID of the first matching subfolder\n",
    "    return None\n",
    "\n",
    "def download_txt_files_from_folder(service, folder_id):\n",
    "    \"\"\"Download all .txt files from a specified folder.\"\"\"\n",
    "    query = f\"'{folder_id}' in parents and mimeType='text/plain'\"\n",
    "    response = service.files().list(q=query, spaces='drive', fields='files(id, name, mimeType)').execute()\n",
    "    files = response.get('files', [])\n",
    "    for file in files:\n",
    "        print(f\"Downloading/exporting {file['name']}...\")\n",
    "        download_or_export_file(service, file['id'], file['name'], file['mimeType'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toy diagnosis data. Might look into a better source, but this will work for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 diagnosis\n",
      "0                  Acute Myeloid Leukaemia\n",
      "1     Adrenocortical Carcinoma (Localised)\n",
      "2    Adrenocortical Carcinoma (Metastatic)\n",
      "3      Adrenocortical Carcinoma (Regional)\n",
      "4                             ALL (B Cell)\n",
      "..                                     ...\n",
      "629                           Typhus Fever\n",
      "630                           Valley Fever\n",
      "631                        West Nile Fever\n",
      "632                           Yellow Fever\n",
      "633                             Zika Fever\n",
      "\n",
      "[634 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# again, if csv exists, load it\n",
    "if os.path.exists('../clean_data/diagnoses.csv'):\n",
    "    diagnoses = pd.read_csv('../clean_data/diagnoses.csv')\n",
    "# otherwise, download it from Google Drive\n",
    "else:\n",
    "    service = create_service()  # Assume this is implemented as shown before\n",
    "    top_level_folder_names = ['Base-Game', 'Mod-Diagnoses']\n",
    "\n",
    "    for folder_name in top_level_folder_names:\n",
    "        folders = find_folders_by_name(service, folder_name)\n",
    "        for folder in folders:\n",
    "            dept_diagnoses_folder_id = find_subfolder_id(service, folder['id'], 'Dept-Diagnoses')\n",
    "            if dept_diagnoses_folder_id:\n",
    "                download_txt_files_from_folder(service, dept_diagnoses_folder_id)\n",
    "    \n",
    "    diagnoses = []\n",
    "    # loop through all files and extract the text following '##' (diagnosis names)\n",
    "    for file in os.listdir('../raw_data/diagnoses'):\n",
    "        with open(f'../raw_data/diagnoses/{file}', 'r') as f:\n",
    "            for line in f:\n",
    "                if '##' in line:\n",
    "                    diagnoses.append(line.split('##')[1].strip())\n",
    "    \n",
    "    diagnoses = pd.DataFrame(diagnoses, columns=['diagnosis']).to_csv('../clean_data/diagnoses.csv', index=False)\n",
    "\n",
    "print(diagnoses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "634 diagnoses to randomize NER training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some dosages are nonsensical, but will hopefully allow the model to generalize well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['930 mL every 9 hours', '830 mg', '290 mg', '565 L', '255 L', '645 g every 20 hours', '270 g', '360 mg', '805 g', '1000 mL', '515 g every 21 hours', '925 L', '540 L', '180 mL', '80 mL', '370 mg every 16 hours', '350 L', '415 mg', '905 mL', '70 g', '875 mg every 42 hours', '285 mg', '475 g', '845 L', '305 mL', '710 mL every 39 hours', '915 g', '390 L', '415 L', '555 mg', '810 g every 41 hours', '735 g', '225 g', '685 g', '540 L', '950 mg as needed', '155 mL', '670 g', '295 g', '635 mg', '125 L every 29 hours', '635 L', '735 mg', '60 mL', '985 mL', '360 mL every 40 hours', '540 mL', '890 mL', '455 L', '125 g', '330 g every 45 hours', '415 L', '940 L', '330 mg', '445 mL', '380 mL every 5 hours', '25 mg', '85 mL', '45 g', '145 L', '840 L every 19 hours', '425 mg', '310 g', '500 L', '330 mg', '760 L every 10 hours', '125 mL', '275 g', '350 L', '900 L', '925 mg every 31 hours', '705 L', '625 g', '500 L', '590 g', '830 L as needed', '555 mL', '520 mg', '685 mg', '865 mg', '975 mg once daily', '600 L', '365 mL', '670 mL', '445 g', '950 mg every 10 hours', '845 L', '695 mL', '860 g', '260 g', '85 g every 9 hours', '900 mL', '855 L', '175 mg', '70 L', '95 mg every 29 hours', '180 g', '230 mL', '615 g', '360 L', '775 L every 12 hours', '395 L', '515 mL', '350 mL', '880 mg', '675 mg every 34 hours', '730 mL', '975 g', '90 g', '405 L', '230 g every 42 hours', '495 L', '165 mL', '410 g', '760 mg', '660 L every 22 hours', '850 g', '875 mg', '795 mg', '630 mg', '570 mL every 47 hours', '145 mg', '655 L', '840 L', '705 mg', '275 mg every 31 hours', '560 mg', '600 g', '285 L', '285 g', '665 mL every 21 hours', '985 g', '345 mg', '115 mL', '870 mL', '655 L every 10 hours', '765 mg', '580 g', '165 g', '800 L', '670 mg once daily', '20 g', '980 mL', '205 mL', '600 mg', '595 L every 27 hours', '290 g', '620 g', '105 mg', '45 mL', '220 mg every 6 hours', '660 mg', '20 L', '630 L', '590 mg', '765 mg every 37 hours', '860 mg', '710 L', '635 L', '935 mL', '445 L every 38 hours', '80 L', '985 mg', '115 mg', '895 mg', '630 mg every 8 hours', '450 mg', '465 mL', '55 mg', '870 L', '765 mL every 15 hours', '730 mL', '695 mL', '355 mg', '995 g', '505 L every 16 hours', '255 mL', '150 mg', '540 mL', '380 mg', '595 L every 15 hours', '110 g', '470 g', '685 g', '985 L', '335 mL every 27 hours', '430 mL', '200 g', '530 mg', '155 L', '125 mL every 35 hours', '155 mg', '190 mL', '155 g', '955 mL', '960 g every 28 hours', '860 g', '175 mg', '935 mL', '570 L', '450 mg as needed', '145 mL', '815 g', '125 L', '425 g', '195 L every 32 hours', '580 mL', '430 g', '820 L', '445 mL', '995 mg every 30 hours', '185 g', '895 L', '700 L', '715 g', '890 g every 8 hours', '180 mg', '875 g', '170 g', '555 mL', '180 g every 28 hours', '780 mL', '210 mg', '55 g', '420 L', '110 mL every 7 hours', '155 mg', '635 g', '810 L', '645 L', '40 g every 31 hours', '280 mL', '515 mL', '135 g', '380 mL', '820 mL every 15 hours', '360 L', '330 mg', '390 mg', '500 L', '620 mg every 28 hours', '300 L', '395 mL', '790 mL', '360 mg', '225 L every 28 hours', '715 mg', '885 g', '380 mL', '285 L', '1000 g every 43 hours', '300 mg', '665 mg', '665 L', '600 g', '310 L every 40 hours', '490 mL', '205 mL', '790 mL', '115 L', '815 g every 44 hours', '500 mg', '870 L', '235 mg', '80 mL', '715 mg every 19 hours', '365 mg', '35 mg', '550 mg', '740 L', '675 L as needed', '590 g', '690 L', '205 g', '925 mg', '600 L every 9 hours', '950 g', '915 mL', '785 L', '670 mg', '335 mL every 17 hours', '305 mg', '375 mL', '765 g', '325 mg', '390 mL every 4 hours', '585 mL', '480 mg', '155 g', '790 g', '605 mg every 4 hours', '415 mL', '595 mg', '730 g', '710 mL', '960 mL every 30 hours', '625 mL', '535 L', '215 mg', '920 L']\n"
     ]
    }
   ],
   "source": [
    "# randomly generate dosage data\n",
    "dosages = []\n",
    "units = ['mg', 'g', 'mL', 'L']\n",
    "concat_every = 5\n",
    "frequency = ['twice daily', 'once daily', 'as needed']\n",
    "\n",
    "for hour in range(48):\n",
    "    frequency.append(f'every {hour} hours')\n",
    "\n",
    "for i in range(300):\n",
    "    dosage = str(random.choice(range(5, 1001, 5))) + f' {random.choice(units)}'\n",
    "    if i % concat_every == 0:\n",
    "        dosage += ' ' + random.choice(frequency)\n",
    "    dosages.append(dosage)\n",
    "\n",
    "print(dosages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if csv exists, load it\n",
    "if os.path.exists('../clean_data/tests.csv'):\n",
    "    tests = pd.read_csv('../clean_data/tests.csv')\n",
    "# otherwise, scrape the data\n",
    "else:\n",
    "    url = \"https://medlineplus.gov/lab-tests/\"\n",
    "\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    tests = []\n",
    "    # select all uorderdered lists with class 'withident breaklist'\n",
    "    for item in soup.select(\".withident.breaklist\"):\n",
    "        tests.append(item.text)\n",
    "\n",
    "    tests = pd.DataFrame(tests, columns=['test'])\n",
    "    tests['test'] = tests['test'].str.split('\\n')\n",
    "    tests = tests.explode('test')\n",
    "    tests = tests[tests['test'] != '']\n",
    "    tests.to_csv('../clean_data/tests.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Symptoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if csv exists, load it\n",
    "if os.path.exists('../clean_data/symptoms.csv'):\n",
    "    symptoms = pd.read_csv('../clean_data/symptoms.csv')\n",
    "else:\n",
    "    symptoms = []\n",
    "    pattern = r\"\\+\\s(.+?)\\s\\(\\d+% of cases \\| .+?\\)\"\n",
    "\n",
    "    # loop through all files and extract the text following '##' (diagnosis names)\n",
    "    for file in os.listdir('../raw_data/diagnoses'):\n",
    "        with open(f'../raw_data/diagnoses/{file}', 'r') as f:\n",
    "            for line in f:\n",
    "                match = re.search(pattern, line)\n",
    "                if match:\n",
    "                    symptoms.append(match.group(1))\n",
    "\n",
    "    symptoms = pd.DataFrame(symptoms, columns=['symptom'])\n",
    "    # make unique\n",
    "    symptoms = symptoms.drop_duplicates()\n",
    "    symptoms.to_csv('../clean_data/symptoms.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['January 01', 'January 02', 'January 03', 'January 04', 'January 05', 'January 06', 'January 07', 'January 08', 'January 09', 'January 10', 'January 11', 'January 12', 'January 13', 'January 14', 'January 15', 'January 16', 'January 17', 'January 18', 'January 19', 'January 20', 'January 21', 'January 22', 'January 23', 'January 24', 'January 25', 'January 26', 'January 27', 'January 28', 'February 01', 'February 02', 'February 03', 'February 04', 'February 05', 'February 06', 'February 07', 'February 08', 'February 09', 'February 10', 'February 11', 'February 12', 'February 13', 'February 14', 'February 15', 'February 16', 'February 17', 'February 18', 'February 19', 'February 20', 'February 21', 'February 22', 'February 23', 'February 24', 'February 25', 'February 26', 'February 27', 'February 28', 'March 01', 'March 02', 'March 03', 'March 04', 'March 05', 'March 06', 'March 07', 'March 08', 'March 09', 'March 10', 'March 11', 'March 12', 'March 13', 'March 14', 'March 15', 'March 16', 'March 17', 'March 18', 'March 19', 'March 20', 'March 21', 'March 22', 'March 23', 'March 24', 'March 25', 'March 26', 'March 27', 'March 28', 'April 01', 'April 02', 'April 03', 'April 04', 'April 05', 'April 06', 'April 07', 'April 08', 'April 09', 'April 10', 'April 11', 'April 12', 'April 13', 'April 14', 'April 15', 'April 16', 'April 17', 'April 18', 'April 19', 'April 20', 'April 21', 'April 22', 'April 23', 'April 24', 'April 25', 'April 26', 'April 27', 'April 28', 'May 01', 'May 02', 'May 03', 'May 04', 'May 05', 'May 06', 'May 07', 'May 08', 'May 09', 'May 10', 'May 11', 'May 12', 'May 13', 'May 14', 'May 15', 'May 16', 'May 17', 'May 18', 'May 19', 'May 20', 'May 21', 'May 22', 'May 23', 'May 24', 'May 25', 'May 26', 'May 27', 'May 28', 'June 01', 'June 02', 'June 03', 'June 04', 'June 05', 'June 06', 'June 07', 'June 08', 'June 09', 'June 10', 'June 11', 'June 12', 'June 13', 'June 14', 'June 15', 'June 16', 'June 17', 'June 18', 'June 19', 'June 20', 'June 21', 'June 22', 'June 23', 'June 24', 'June 25', 'June 26', 'June 27', 'June 28', 'July 01', 'July 02', 'July 03', 'July 04', 'July 05', 'July 06', 'July 07', 'July 08', 'July 09', 'July 10', 'July 11', 'July 12', 'July 13', 'July 14', 'July 15', 'July 16', 'July 17', 'July 18', 'July 19', 'July 20', 'July 21', 'July 22', 'July 23', 'July 24', 'July 25', 'July 26', 'July 27', 'July 28', 'August 01', 'August 02', 'August 03', 'August 04', 'August 05', 'August 06', 'August 07', 'August 08', 'August 09', 'August 10', 'August 11', 'August 12', 'August 13', 'August 14', 'August 15', 'August 16', 'August 17', 'August 18', 'August 19', 'August 20', 'August 21', 'August 22', 'August 23', 'August 24', 'August 25', 'August 26', 'August 27', 'August 28', 'September 01', 'September 02', 'September 03', 'September 04', 'September 05', 'September 06', 'September 07', 'September 08', 'September 09', 'September 10', 'September 11', 'September 12', 'September 13', 'September 14', 'September 15', 'September 16', 'September 17', 'September 18', 'September 19', 'September 20', 'September 21', 'September 22', 'September 23', 'September 24', 'September 25', 'September 26', 'September 27', 'September 28', 'October 01', 'October 02', 'October 03', 'October 04', 'October 05', 'October 06', 'October 07', 'October 08', 'October 09', 'October 10', 'October 11', 'October 12', 'October 13', 'October 14', 'October 15', 'October 16', 'October 17', 'October 18', 'October 19', 'October 20', 'October 21', 'October 22', 'October 23', 'October 24', 'October 25', 'October 26', 'October 27', 'October 28', 'November 01', 'November 02', 'November 03', 'November 04', 'November 05', 'November 06', 'November 07', 'November 08', 'November 09', 'November 10', 'November 11', 'November 12', 'November 13', 'November 14', 'November 15', 'November 16', 'November 17', 'November 18', 'November 19', 'November 20', 'November 21', 'November 22', 'November 23', 'November 24', 'November 25', 'November 26', 'November 27', 'November 28', 'December 01', 'December 02', 'December 03', 'December 04', 'December 05', 'December 06', 'December 07', 'December 08', 'December 09', 'December 10', 'December 11', 'December 12', 'December 13', 'December 14', 'December 15', 'December 16', 'December 17', 'December 18', 'December 19', 'December 20', 'December 21', 'December 22', 'December 23', 'December 24', 'December 25', 'December 26', 'December 27', 'December 28']\n"
     ]
    }
   ],
   "source": [
    "dates = []\n",
    "\n",
    "# generate dates in the format \"Month Day\"\n",
    "for i in range(1, 13):\n",
    "    for j in range(1, 29):\n",
    "        # year is arbitrary\n",
    "        date = datetime.date(2021, i, j)\n",
    "        full_date = f\"{date.strftime('%B')} {date.strftime('%d')}\"\n",
    "        dates.append(full_date)\n",
    "\n",
    "print(dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = []\n",
    "# generate times in the format \"Hour AM/PM\"\n",
    "for i in range(1, 13):\n",
    "    for j in [\"AM\", \"PM\"]:\n",
    "        time = f\"{i} {j}\"\n",
    "        times.append(time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Body parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hard-coding for now. Will change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anatomies = [\"left femur\", \"right knee\", \"abdominal region\", \"left lung\", \"right lung\", \"left kidney\", \"right kidney\",\n",
    "             \"left eye\", \"right eye\", \"left ear\", \"right ear\", \"left hand\", \"right hand\", \"left foot\", \"right foot\",\n",
    "             \"left arm\", \"right arm\", \"left leg\", \"right leg\", \"left shoulder\", \"right shoulder\", \"left hip\", \"right hip\",\n",
    "             \"left elbow\", \"right elbow\", \"left wrist\", \"right wrist\", \"left ankle\", \"right ankle\", \"left toe\", \"right toe\",\n",
    "             \"left finger\", \"right finger\", \"left thumb\", \"right thumb\", \"left nostril\", \"right nostril\", \"left cheek\", \"right cheek\",\n",
    "             \"left temple\", \"right temple\", \"left jaw\", \"right jaw\", \"left chin\", \"right chin\", \"left neck\", \"right neck\", \"left collarbone\",\n",
    "             \"right collarbone\", \"left rib\", \"right rib\", \"left hip bone\", \"right hip bone\", \"left thigh\", \"right thigh\", \"left calf\",\n",
    "             \"right calf\", \"left shin\", \"right shin\", \"left heel\", \"right heel\", \"left sole\", \"right sole\", \"left toe\", \"right toe\",\n",
    "             \"left finger\", \"right finger\", \"left thumb\", \"right thumb\", \"left palm\", \"right palm\", \"left wrist\", \"right wrist\", \"left forearm\",\n",
    "             \"right forearm\", \"left bicep\", \"right bicep\", \"left tricep\", \"right tricep\", \"left shoulder\", \"right shoulder\", \"left chest\", \"right chest\",\n",
    "             \"left breast\", \"right breast\", \"left nipple\", \"right nipple\", \"left rib\", \"right rib\", \"left abdomen\", \"right abdomen\", \"left hip\",\n",
    "             \"right hip\", \"left groin\", \"right groin\", \"left thigh\", \"right thigh\", \"left knee\", \"right knee\", \"left shin\", \"right shin\", \"left calf\",\n",
    "             \"right calf\", \"left ankle\", \"right ankle\", \"left foot\", \"right foot\", \"left toe\", \"right toe\", \"left finger\", \"right finger\", \"left thumb\",\n",
    "             \"right thumb\", \"left hand\", \"right hand\", \"left wrist\", \"right wrist\", \"left forearm\", \"right forearm\", \"left elbow\", \"right elbow\",\n",
    "             \"left upper arm\", \"right upper arm\", \"left shoulder\", \"heart\", \"liver\", \"stomach\", \"intestines\", \"pancreas\", \"spleen\", \"bladder\", \"esophagus\"]\n",
    "\n",
    "len(anatomies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make unique\n",
    "anatomies = list(set(anatomies))\n",
    "len(anatomies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ToDo: Need to combine logic of next two cells to only generate a train_data.csv instead of two files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a random date\n",
    "def generate_example():\n",
    "    output = {}\n",
    "    diagnosis = random.choice(diagnoses['diagnosis'].values)\n",
    "    medication = random.choice(medications['medication'].values)\n",
    "    dosage = random.choice(dosages)\n",
    "    test_name = random.choice(tests['test'].values)\n",
    "    symptom = random.choice(symptoms['symptom'].values)\n",
    "    body_part = random.choice(anatomies)\n",
    "    time = random.choice(times)\n",
    "    date = random.choice(dates)\n",
    "\n",
    "    choices = [diagnosis, medication, dosage, test_name, symptom, body_part, time, date]\n",
    "    choice_map = ['diagnosis', 'medication', 'dosage', 'test_name', 'symptom', 'body_part', 'time', 'date']\n",
    "    entities = []\n",
    "\n",
    "    text_elements = [\n",
    "        f\"The patient was diagnosed with {diagnosis} last year.\",\n",
    "        f\"He has been prescribed {medication} {dosage}.\",\n",
    "        f\"A follow-up appointment is scheduled for {time} on {date}.\",\n",
    "        f\"{test_name} measurements indicate {diagnosis}.\",\n",
    "        f\"The {test_name} revealed a {diagnosis} in the {body_part}.\",\n",
    "        f\"Patient presents with {symptom}.\",\n",
    "        f\"Prescribe {dosage} of {medication} for pain relief.\",\n",
    "        f\"The {test_name} shows normal {body_part} function.\",\n",
    "        f\"She mentioned an allergy to {medication}.\",\n",
    "        f\"Examine the {symptom} in the patient's {body_part}.\",\n",
    "    ]\n",
    "    text = random.choice(text_elements)\n",
    "    # Simplified entity recognition (just a placeholder, not accurate)\n",
    "    for index, choice in enumerate(choices):\n",
    "        if text.find(choice) != -1:\n",
    "            entities.append({\"start\": text.find(choice), \"end\": text.find(choice) + len(choice), \"label\": f\"{choice_map[index]}\"})\n",
    "    \n",
    "    output[\"text\"] = text\n",
    "    output[\"entities\"] = entities\n",
    "    return output\n",
    "\n",
    "# Generate 1000 examples\n",
    "train_data = [generate_example() for _ in range(1000)]\n",
    "\n",
    "# Save the examples to a JSON file\n",
    "file_path = '../clean_data/train_data.json'\n",
    "with open(file_path, 'w') as file:\n",
    "    json.dump(train_data, file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON data\n",
    "with open('../clean_data/train_data.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Convert the JSON data to the desired format\n",
    "formatted_data = []\n",
    "for item in data:\n",
    "    text = item['text']\n",
    "    entities = []\n",
    "    for entity in item['entities']:\n",
    "        start = entity['start']\n",
    "        end = entity['end']\n",
    "        label = entity['label']\n",
    "        entities.append((start, end, label.upper()))  # Convert label to uppercase as shown in the example\n",
    "    formatted_data.append((text, {\"entities\": entities}))\n",
    "\n",
    "# Convert to DataFrame for easy CSV saving\n",
    "df = pd.DataFrame(formatted_data, columns=['Text', 'Entities'])\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('../clean_data/formatted_train_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DIAGNOSIS', 'MEDICATION', 'DATE', 'TEST_NAME', 'BODY_PART', 'DOSAGE', 'SYMPTOM', 'TIME'}\n"
     ]
    }
   ],
   "source": [
    "entity_labels = set()\n",
    "for text, annotations in formatted_data:\n",
    "    for entity in annotations['entities']:\n",
    "        entity_labels.add(entity[2])\n",
    "\n",
    "print(entity_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('../clean_data/models/ner_model'):\n",
    "    nlp = spacy.load('../clean_data/models/ner_model')\n",
    "else:\n",
    "    # Load a blank model\n",
    "    nlp = spacy.blank('en')\n",
    "\n",
    "    # Add the NER pipeline if not already present\n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe('ner')\n",
    "    else:\n",
    "        ner = nlp.get_pipe('ner')\n",
    "\n",
    "    # Add entity labels to the model\n",
    "    for entity in entity_labels:\n",
    "        ner.add_label(entity)\n",
    "\n",
    "    # Train model\n",
    "    optimizer = nlp.begin_training()\n",
    "    for itn in range(10):  # Number of training iterations\n",
    "        for text, annotations in formatted_data:\n",
    "            doc = nlp.make_doc(text)\n",
    "            example = Example.from_dict(doc, annotations)\n",
    "            nlp.update([example], drop=0.5, sgd=optimizer)\n",
    "\n",
    "    # Save the model\n",
    "    nlp.to_disk('../clean_data/models/ner_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will need more rigorous testing/improvement im sure, but fine for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500mg DOSAGE\n",
      "Ibuprofen MEDICATION\n"
     ]
    }
   ],
   "source": [
    "# Example text\n",
    "text = \"Patient was administered 500mg of Ibuprofen.\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Iterate over the predicted entities\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
