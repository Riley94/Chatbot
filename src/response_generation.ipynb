{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import spacy\n",
    "from helpers import RNN, predict, load_data\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_model = torch.load('../clean_data/models/intents_classifier.pth')\n",
    "entity_model = spacy.load('../clean_data/models/ner_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "> hello there\n",
      "(-0.01) greeting\n",
      "(-6.91) thanks\n",
      "(-7.01) pharmacy_search\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[-0.007337289396673441, 'greeting'],\n",
       " [-6.91290283203125, 'thanks'],\n",
       " [-7.01064395904541, 'pharmacy_search']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('hello there', intent_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intents_and_entites(text):\n",
    "    intent_prob = -np.inf\n",
    "    intents_pred = predict(text, intent_model)\n",
    "    intent = ''\n",
    "\n",
    "    for item in intents_pred:\n",
    "        if item[0] > intent_prob:\n",
    "            intent_prob = item[0]\n",
    "            intent = item[1]\n",
    "    ner_doc = entity_model(text)\n",
    "    entities = [(ent.text, ent.label_) for ent in ner_doc.ents]\n",
    "    return intent, entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "> The patient was prescribed 500 mg of Metformin.\n",
      "(-1.62) goodbye\n",
      "(-1.94) blood_pressure_search\n",
      "(-2.06) pharmacy_search\n",
      "goodbye\n",
      "[('500 mg', 'DOSAGE'), ('Metformin', 'MEDICATION')]\n"
     ]
    }
   ],
   "source": [
    "text = \"The patient was prescribed 500 mg of Metformin.\"\n",
    "intent, entities = get_intents_and_entites(text)\n",
    "print(intent)\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly still needs work to make the classifications better. NER does well with recognizing med names and dosages. But will also need improved in other areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "intents_dict, response_dict = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'greeting': ['Hello, thanks for asking', 'Good to see you again', 'Hi there, how can I help?'], 'goodbye': ['See you!', 'Have a nice day', 'Bye! Come back again soon.'], 'thanks': ['Happy to help!', 'Any time!', 'My pleasure'], 'noanswer': [\"Sorry, can't understand you\", 'Please give me more info', 'Not sure I understand'], 'options': ['I can guide you through Adverse drug reaction list, Blood pressure tracking, Hospitals and Pharmacies', 'Offering support for Adverse drug reaction, Blood pressure, Hospitals and Pharmacies'], 'adverse_drug': ['Navigating to Adverse drug reaction module'], 'blood_pressure': ['Navigating to Blood Pressure module'], 'blood_pressure_search': ['Please provide Patient ID', 'Patient ID?'], 'search_blood_pressure_by_patient_id': ['Loading Blood pressure result for Patient'], 'pharmacy_search': ['Please provide pharmacy name'], 'search_pharmacy_by_name': ['Loading pharmacy details'], 'hospital_search': ['Please provide hospital name or location'], 'search_hospital_by_params': ['Please provide hospital type'], 'search_hospital_by_type': ['Loading hospital details']}\n"
     ]
    }
   ],
   "source": [
    "print(response_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source of pre-trained model: https://huggingface.co/openai-community/gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/riley/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Hello, I'm a language model, but what I'm really doing is making a human-readable document. There are other languages, but those are\"},\n",
       " {'generated_text': \"Hello, I'm a language model, not a syntax model. That's why I like it. I've done a lot of programming projects.\\n\"},\n",
       " {'generated_text': \"Hello, I'm a language model, and I'll do it in no time!\\n\\nOne of the things we learned from talking to my friend\"},\n",
       " {'generated_text': \"Hello, I'm a language model, not a command line tool.\\n\\nIf my code is simple enough:\\n\\nif (use (string\"},\n",
       " {'generated_text': \"Hello, I'm a language model, I've been using Language in all my work. Just a small example, let's see a simplified example.\"}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(42)\n",
    "generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "from gpt4all import GPT4All\n",
    "model = GPT4All(\"orca-mini-3b-gguf2-q4_0.gguf\")\n",
    "output = model.generate(\"The capital of France is \", max_tokens=3)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hello, I'm doing well. How about you?\n"
     ]
    }
   ],
   "source": [
    "print(model.generate(\"Hello, how are you?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I'm sorry, but I can't seem to find your profile. Could you please provide me with some more information so that I can assist you better?\n",
      "\n",
      "I have a profile on LinkedIn. What can I do with it?\n",
      "\n",
      "The information you are looking for is not provided in the question. Please provide more context or information so that I can better assist you.\n",
      "\n",
      "I am a software engineer.\n",
      ".\n",
      "I'm sorry, but I can't believe that.\n",
      "You know, it's not like I'm some kind of\n",
      "superhero or anything.\n",
      "I just try to do my best and be a good person.\n",
      "And if I happen to help someone along the way,\n",
      "that's just a nice bonus.\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    text = input(\"Enter a message: \")\n",
    "    if text == 'exit':\n",
    "        break\n",
    "    else:\n",
    "        print(model.generate(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "goodbye\n",
      "[('500 mg', 'DOSAGE'), ('Metformin', 'MEDICATION')]\n"
     ]
    }
   ],
   "source": [
    "print(intent)\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The patient has not reported any side effects from the medication.\n"
     ]
    }
   ],
   "source": [
    "# Construct a prompt for gpt4all\n",
    "input_text = \"The patient was prescribed 500 mg of Metformin.\"\n",
    "intent\n",
    "contextual_prompt = f\"Intent: {intent}, Entities: {', '.join([f'{entity[1]}: {entity[0]}' for entity in entities])}, Respond to: {input_text}\"\n",
    "print(model.generate(contextual_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'order': 'a',\n",
       "  'md5sum': 'f692417a22405d80573ac10cb0cd6c6a',\n",
       "  'name': 'Mistral OpenOrca',\n",
       "  'filename': 'mistral-7b-openorca.gguf2.Q4_0.gguf',\n",
       "  'filesize': '4108928128',\n",
       "  'requires': '2.5.0',\n",
       "  'ramrequired': '8',\n",
       "  'parameters': '7 billion',\n",
       "  'quant': 'q4_0',\n",
       "  'type': 'Mistral',\n",
       "  'description': '<strong>Best overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href=\"https://atlas.nomic.ai/\">Nomic Atlas</a><li>Licensed for commercial use</ul>',\n",
       "  'url': 'https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf',\n",
       "  'promptTemplate': '<|im_start|>user\\n%1<|im_end|>\\n<|im_start|>assistant\\n',\n",
       "  'systemPrompt': '<|im_start|>system\\nYou are MistralOrca, a large language model trained by Alignment Lab AI. For multi-step problems, write out your reasoning for each step.\\n<|im_end|>'},\n",
       " {'order': 'b',\n",
       "  'md5sum': '97463be739b50525df56d33b26b00852',\n",
       "  'name': 'Mistral Instruct',\n",
       "  'filename': 'mistral-7b-instruct-v0.1.Q4_0.gguf',\n",
       "  'filesize': '4108916384',\n",
       "  'requires': '2.5.0',\n",
       "  'ramrequired': '8',\n",
       "  'parameters': '7 billion',\n",
       "  'quant': 'q4_0',\n",
       "  'type': 'Mistral',\n",
       "  'systemPrompt': ' ',\n",
       "  'description': '<strong>Best overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>',\n",
       "  'url': 'https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf',\n",
       "  'promptTemplate': '[INST] %1 [/INST]'},\n",
       " {'order': 'c',\n",
       "  'md5sum': 'c4c78adf744d6a20f05c8751e3961b84',\n",
       "  'name': 'GPT4All Falcon',\n",
       "  'filename': 'gpt4all-falcon-newbpe-q4_0.gguf',\n",
       "  'filesize': '4210994112',\n",
       "  'requires': '2.6.0',\n",
       "  'ramrequired': '8',\n",
       "  'parameters': '7 billion',\n",
       "  'quant': 'q4_0',\n",
       "  'type': 'Falcon',\n",
       "  'systemPrompt': ' ',\n",
       "  'description': '<strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>',\n",
       "  'url': 'https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf',\n",
       "  'promptTemplate': '### Instruction:\\n%1\\n### Response:\\n'},\n",
       " {'order': 'e',\n",
       "  'md5sum': '00c8593ba57f5240f59662367b3ed4a5',\n",
       "  'name': 'Orca 2 (Medium)',\n",
       "  'filename': 'orca-2-7b.Q4_0.gguf',\n",
       "  'filesize': '3825824192',\n",
       "  'requires': '2.5.2',\n",
       "  'ramrequired': '8',\n",
       "  'parameters': '7 billion',\n",
       "  'quant': 'q4_0',\n",
       "  'type': 'LLaMA2',\n",
       "  'systemPrompt': ' ',\n",
       "  'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>',\n",
       "  'url': 'https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf'},\n",
       " {'order': 'f',\n",
       "  'md5sum': '3c0d63c4689b9af7baa82469a6f51a19',\n",
       "  'name': 'Orca 2 (Full)',\n",
       "  'filename': 'orca-2-13b.Q4_0.gguf',\n",
       "  'filesize': '7365856064',\n",
       "  'requires': '2.5.2',\n",
       "  'ramrequired': '16',\n",
       "  'parameters': '13 billion',\n",
       "  'quant': 'q4_0',\n",
       "  'type': 'LLaMA2',\n",
       "  'systemPrompt': ' ',\n",
       "  'description': '<ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>',\n",
       "  'url': 'https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf'},\n",
       " {'order': 'g',\n",
       "  'md5sum': '5aff90007499bce5c64b1c0760c0b186',\n",
       "  'name': 'Wizard v1.2',\n",
       "  'filename': 'wizardlm-13b-v1.2.Q4_0.gguf',\n",
       "  'filesize': '7365834624',\n",
       "  'requires': '2.5.0',\n",
       "  'ramrequired': '16',\n",
       "  'parameters': '13 billion',\n",
       "  'quant': 'q4_0',\n",
       "  'type': 'LLaMA2',\n",
       "  'systemPrompt': ' ',\n",
       "  'description': '<strong>Best overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>',\n",
       "  'url': 'https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf'},\n",
       " {'order': 'h',\n",
       "  'md5sum': '3d12810391d04d1153b692626c0c6e16',\n",
       "  'name': 'Hermes',\n",
       "  'filename': 'nous-hermes-llama2-13b.Q4_0.gguf',\n",
       "  'filesize': '7366062080',\n",
       "  'requires': '2.5.0',\n",
       "  'ramrequired': '16',\n",
       "  'parameters': '13 billion',\n",
       "  'quant': 'q4_0',\n",
       "  'type': 'LLaMA2',\n",
       "  'systemPrompt': ' ',\n",
       "  'description': '<strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>',\n",
       "  'url': 'https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf',\n",
       "  'promptTemplate': '### Instruction:\\n%1\\n### Response:\\n'},\n",
       " {'order': 'i',\n",
       "  'md5sum': '40388eb2f8d16bb5d08c96fdfaac6b2c',\n",
       "  'name': 'Snoozy',\n",
       "  'filename': 'gpt4all-13b-snoozy-q4_0.gguf',\n",
       "  'filesize': '7365834624',\n",
       "  'requires': '2.5.0',\n",
       "  'ramrequired': '16',\n",
       "  'parameters': '13 billion',\n",
       "  'quant': 'q4_0',\n",
       "  'type': 'LLaMA',\n",
       "  'systemPrompt': ' ',\n",
       "  'description': '<strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>',\n",
       "  'url': 'https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf'},\n",
       " {'order': 'j',\n",
       "  'md5sum': '15dcb4d7ea6de322756449c11a0b7545',\n",
       "  'name': 'MPT Chat',\n",
       "  'filename': 'mpt-7b-chat-newbpe-q4_0.gguf',\n",
       "  'filesize': '3912373472',\n",
       "  'requires': '2.6.0',\n",
       "  'ramrequired': '8',\n",
       "  'parameters': '7 billion',\n",
       "  'quant': 'q4_0',\n",
       "  'type': 'MPT',\n",
       "  'description': '<strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>',\n",
       "  'url': 'https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf',\n",
       "  'promptTemplate': '<|im_start|>user\\n%1<|im_end|>\\n<|im_start|>assistant\\n',\n",
       "  'systemPrompt': '<|im_start|>system\\n- You are a helpful assistant chatbot trained by MosaicML.\\n- You answer questions.\\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>'},\n",
       " {'order': 'k',\n",
       "  'md5sum': '0e769317b90ac30d6e09486d61fefa26',\n",
       "  'name': 'Mini Orca (Small)',\n",
       "  'filename': 'orca-mini-3b-gguf2-q4_0.gguf',\n",
       "  'filesize': '1979946720',\n",
       "  'requires': '2.5.0',\n",
       "  'ramrequired': '4',\n",
       "  'parameters': '3 billion',\n",
       "  'quant': 'q4_0',\n",
       "  'type': 'OpenLLaMa',\n",
       "  'description': '<strong>Small version of new model with novel dataset</strong><br><ul><li>Instruction based<li>Explain tuned datasets<li>Orca Research Paper dataset construction approaches<li>Cannot be used commercially</ul>',\n",
       "  'url': 'https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf',\n",
       "  'promptTemplate': '### User:\\n%1\\n### Response:\\n',\n",
       "  'systemPrompt': '### System:\\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\\n\\n'},\n",
       " {'order': 'l',\n",
       "  'md5sum': 'c232f17e09bca4b7ee0b5b1f4107c01e',\n",
       "  'disableGUI': 'true',\n",
       "  'name': 'Replit',\n",
       "  'filename': 'replit-code-v1_5-3b-newbpe-q4_0.gguf',\n",
       "  'filesize': '1953055104',\n",
       "  'requires': '2.6.0',\n",
       "  'ramrequired': '4',\n",
       "  'parameters': '3 billion',\n",
       "  'quant': 'q4_0',\n",
       "  'type': 'Replit',\n",
       "  'systemPrompt': ' ',\n",
       "  'promptTemplate': '%1',\n",
       "  'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>',\n",
       "  'url': 'https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf'},\n",
       " {'order': 'm',\n",
       "  'md5sum': '70841751ccd95526d3dcfa829e11cd4c',\n",
       "  'disableGUI': 'true',\n",
       "  'name': 'Starcoder',\n",
       "  'filename': 'starcoder-newbpe-q4_0.gguf',\n",
       "  'filesize': '8987411904',\n",
       "  'requires': '2.6.0',\n",
       "  'ramrequired': '4',\n",
       "  'parameters': '7 billion',\n",
       "  'quant': 'q4_0',\n",
       "  'type': 'Starcoder',\n",
       "  'systemPrompt': ' ',\n",
       "  'promptTemplate': '%1',\n",
       "  'description': '<strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>',\n",
       "  'url': 'https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf'},\n",
       " {'order': 'n',\n",
       "  'md5sum': 'e973dd26f0ffa6e46783feaea8f08c83',\n",
       "  'disableGUI': 'true',\n",
       "  'name': 'Rift coder',\n",
       "  'filename': 'rift-coder-v0-7b-q4_0.gguf',\n",
       "  'filesize': '3825903776',\n",
       "  'requires': '2.5.0',\n",
       "  'ramrequired': '8',\n",
       "  'parameters': '7 billion',\n",
       "  'quant': 'q4_0',\n",
       "  'type': 'LLaMA',\n",
       "  'systemPrompt': ' ',\n",
       "  'promptTemplate': '%1',\n",
       "  'description': '<strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>',\n",
       "  'url': 'https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf'},\n",
       " {'order': 'o',\n",
       "  'md5sum': 'e479e6f38b59afc51a470d1953a6bfc7',\n",
       "  'disableGUI': 'true',\n",
       "  'name': 'SBert',\n",
       "  'filename': 'all-MiniLM-L6-v2-f16.gguf',\n",
       "  'filesize': '45887744',\n",
       "  'requires': '2.5.0',\n",
       "  'ramrequired': '1',\n",
       "  'parameters': '40 million',\n",
       "  'quant': 'f16',\n",
       "  'type': 'Bert',\n",
       "  'systemPrompt': ' ',\n",
       "  'description': '<strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)',\n",
       "  'url': 'https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf'},\n",
       " {'order': 'p',\n",
       "  'md5sum': '919de4dd6f25351bcb0223790db1932d',\n",
       "  'name': 'EM German Mistral',\n",
       "  'filename': 'em_german_mistral_v01.Q4_0.gguf',\n",
       "  'filesize': '4108916352',\n",
       "  'requires': '2.5.0',\n",
       "  'ramrequired': '8',\n",
       "  'parameters': '7 billion',\n",
       "  'quant': 'q4_0',\n",
       "  'type': 'Mistral',\n",
       "  'description': '<strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>',\n",
       "  'url': 'https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf',\n",
       "  'promptTemplate': 'USER: %1 ASSISTANT: ',\n",
       "  'systemPrompt': 'Du bist ein hilfreicher Assistent. '}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/riley/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading shards: 100%|██████████| 3/3 [04:23<00:00, 87.80s/it] \n",
      "Loading checkpoint shards:  33%|███▎      | 1/3 [02:00<04:01, 120.82s/it]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"pankajmathur/orca_mini_3b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"pankajmathur/orca_mini_3b\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.to(\"cuda:0\")\n",
    "else:\n",
    "    model.to(\"cpu\")\n",
    "\n",
    "prompt = \"Describe a painting of a falcon in a very detailed way.\" # Change this to your prompt\n",
    "prompt_template = f\"### Instruction: {prompt}\\n### Response:\"\n",
    "\n",
    "tokens = tokenizer(prompt_template, return_tensors=\"pt\").input_ids.to(\"cuda:0\")\n",
    "output = model.generate(input_ids=tokens, max_new_tokens=256, do_sample=True, temperature=0.8)\n",
    "\n",
    "# Print the generated text\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = {}\n",
    "index2word = {}\n",
    "EOS_token = 1\n",
    "SOS_token = 0\n",
    "intents_dict, responses_dict = load_data()\n",
    "\n",
    "def indexesFromSentence(sentence):\n",
    "    return [word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def tensorFromSentence(sentence):\n",
    "    indexes = indexesFromSentence(sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(pair[0])\n",
    "    target_tensor = tensorFromSentence(pair[1])\n",
    "    return (input_tensor, target_tensor)\n",
    "\n",
    "def get_dataloader(batch_size):\n",
    "    input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
    "\n",
    "    n = len(pairs)\n",
    "    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "\n",
    "    for idx, (inp, tgt) in enumerate(pairs):\n",
    "        inp_ids = indexesFromSentence(inp)\n",
    "        tgt_ids = indexesFromSentence(tgt)\n",
    "        inp_ids.append(EOS_token)\n",
    "        tgt_ids.append(EOS_token)\n",
    "        input_ids[idx, :len(inp_ids)] = inp_ids\n",
    "        target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
    "\n",
    "    train_data = TensorDataset(torch.LongTensor(input_ids).to(device),\n",
    "                               torch.LongTensor(target_ids).to(device))\n",
    "\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "    return input_lang, output_lang, train_dataloader"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
